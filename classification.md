# AI Classification Methodology

## Overview

This project uses a structured approach to AI classification with SQLite3 as the central data store. Each classifier produces traceable, versioned outputs that can be reviewed, iterated, and improved.

---

## Naming Conventions

### Tables

| Prefix | Type | Example |
|--------|------|---------|
| `0_` | Raw data (original source) | `0_raw_data` |
| `1_`, `2_`, ... | Processing steps (numbered in order) | `1_discussion_messages` |
| `X_CLASSIFIER_*` | Tables containing AI-generated columns | `2_CLASSIFIER_name_extraction` |

### Columns

AI-generated columns are tagged with the classifier ID that produced them:

```
column_name [C1]    → Generated by classifier C1
column_name [C2]    → Generated by classifier C2
```

This makes it immediately clear which columns are AI outputs and which classifier created them.

---

## Classifier Registry

All classifiers are registered in the `ai_classification_runs` table:

| Column | Description |
|--------|-------------|
| `run_id` | Unique identifier (e.g., `C1`, `C2`) |
| `run_name` | Descriptive name (e.g., `name_extraction_v1`) |
| `run_type` | Category: `extraction`, `consolidation`, `filtering`, `relationship`, `clustering` |
| `model_used` | LLM model (e.g., `gemini-3.0-flash`, `gpt-4o`) |
| `prompt_used` | Full prompt template |
| `input_columns` | JSON array of input column paths |
| `output_columns` | JSON array of output column paths |
| `created_at` | Timestamp |
| `total_cost` | Total API cost in USD for this run |
| `notes` | Description of what the classifier does |

---

## Creating a New Classifier

### Step 1: Define the classifier

Before coding, answer these questions:

1. **What is the classifier ID?** → Use next available `CX` (e.g., `C6`)
2. **What table will store the results?** → Create or use existing table with `CLASSIFIER` prefix
3. **What are the input columns?** → List the columns the AI will read
4. **What are the output columns?** → Define new columns with `[CX]` suffix
5. **What model will you use?** → Choose based on complexity/cost

### Step 2: Create/modify the table

```sql
-- Example: Adding a new classifier output to existing table
ALTER TABLE "4_CLASSIFIER_relationship_extraction"
ADD COLUMN "sentiment [C6]" TEXT;

-- Example: Creating a new classifier table
CREATE TABLE IF NOT EXISTS "8_CLASSIFIER_topic_classification" (
    thread_id TEXT PRIMARY KEY,
    body TEXT,
    "topic [C6]" TEXT,
    "confidence [C6]" REAL
);
```

### Step 3: Register the classifier

```sql
INSERT INTO ai_classification_runs (
    run_id, run_name, run_type, model_used,
    prompt_used, input_columns, output_columns, total_cost, notes
) VALUES (
    'C6',
    'topic_classification_v1',
    'classification',
    'gemini-3.0-flash',
    'Your prompt here...',
    '["8_CLASSIFIER_topic_classification.body"]',
    '["8_CLASSIFIER_topic_classification.topic [C6]"]',
    0.0,  -- Updated after run completes
    'Classifies email threads by topic'
);
```

### Step 4: Run the classifier

```python
# Pseudocode structure
for row in input_data:
    result = call_llm(prompt, row)
    save_to_table(table, row_id, {"topic [C6]": result})
```

---

## Quality Control Workflow

### 1. Sample Review

After running a classifier, review a random sample:

```sql
-- Get 10 random samples to review
SELECT thread_id, body, "topic [C6]"
FROM "8_CLASSIFIER_topic_classification"
ORDER BY RANDOM()
LIMIT 10;
```

### 2. Iteration

If quality is insufficient:

1. **Version the classifier**: `C6` → `C6_v2` (or create new `C7`)
2. **Keep old results**: Add new column rather than overwriting
3. **Update the prompt**: Modify based on failure patterns
4. **Re-run**: Process again with improved prompt

```sql
-- Add new version column
ALTER TABLE "8_CLASSIFIER_topic_classification"
ADD COLUMN "topic [C6_v2]" TEXT;
```

### 3. Comparison

Compare versions side-by-side:

```sql
SELECT thread_id, "topic [C6]", "topic [C6_v2]"
FROM "8_CLASSIFIER_topic_classification"
WHERE "topic [C6]" != "topic [C6_v2]";
```

---

## Best Practices

1. **Never overwrite**: Add new columns for iterations, don't delete old ones
2. **Always register**: Every classifier must be in `ai_classification_runs`
3. **Tag outputs**: Every AI column must have `[CX]` suffix
4. **Document prompts**: Store the exact prompt used in the registry
5. **Number tables**: Keep processing order clear with numeric prefixes
6. **Use CLASSIFIER prefix**: Makes AI output tables immediately identifiable

---

## Example Pipeline

```
0_raw_data (source parquet)
    ↓
1_discussion_messages (parsed emails)
    ↓
2_CLASSIFIER_name_extraction [C1] → extracts names
    ↓
3_CLASSIFIER_name_consolidation [C2] → deduplicates names
    ↓
4_CLASSIFIER_relationship_extraction [C4] → describes relationships
    ↓
5_network_edges → graph structure (computed, not AI)
    ↓
6_CLASSIFIER_cluster_analysis [C5] → analyzes communities
```

---

## Sample Workflow

Before running classification on the full dataset, always test on a sample first.

### Sample Tables

Sample tables use the `SAMPLE_` prefix:
- `1_discussion_messages` → `SAMPLE_1_discussion_messages`
- `2_CLASSIFIER_name_extraction` → `SAMPLE_2_CLASSIFIER_name_extraction`

### File Export

All classification results are saved to `data/classification/` with naming:
```
{run_id}_{run_name}_{version}.json
```
Example: `C6_sentiment_classification_v1.json`

### Workflow

1. Create sample table (e.g., 500 rows) from source table
2. Run classifier on sample table
3. Review results and iterate on prompt if needed
4. Once satisfied, run on full dataset

---

## Instructions for Claude

When asked to create or run an AI classification task, Claude MUST follow this procedure autonomously:

### Before Running Classification

1. **Analyze the database schema silently**
   - Run `.tables` and `.schema` to understand structure
   - Check `ai_classification_runs` for existing classifiers
   - Identify the next available classifier ID (`CX`)

2. **Infer the right columns automatically**
   - Based on the user's request, deduce which input columns are needed
   - Look at existing table structures and column names
   - Choose the most logical table and columns without asking
   - Only ask if genuinely ambiguous (multiple valid options)

3. **Propose a plan, don't ask questions**
   - Present what you WILL do, not what you COULD do
   - Example: "I'll create classifier C6 using `body` from `1_discussion_messages`"
   - User can correct if wrong, but default to action

### Running Classification

4. **Create/modify the table first**
   - Add output columns with `[CX]` suffix
   - Use `CLASSIFIER` prefix in table name if new table

5. **Register the classifier BEFORE running**
   - Insert into `ai_classification_runs` with all metadata
   - Include the full prompt, input/output columns, model used

6. **Run with progress tracking and optimization**
   - Always use `tqdm` for progress bars
   - **Batch requests**: Group multiple items per API call when possible (e.g., 10-20 items per batch)
   - **Multiprocess**: Use `concurrent.futures.ThreadPoolExecutor` for parallel API calls
   - **Async when possible**: Use `asyncio` with `aiohttp` for maximum throughput
   - Save results incrementally to database
   - Recommended: 10-50 concurrent requests depending on API rate limits

7. **Export results to file**
   - Save to `data/classification/{run_id}_{run_name}.json`
   - Include all rows with input and output columns
   - This allows external review and backup

8. **Track API costs**
   - Count input/output tokens for each API call
   - Calculate total cost based on model pricing
   - Update `total_cost` in `ai_classification_runs` after completion
   - Display final cost to user

### After Classification

9. **Show sample results and cost**
   - Display 5-10 random samples to the user
   - Query: `SELECT * FROM table ORDER BY RANDOM() LIMIT 10`
   - Report total API cost: "Total cost: $X.XX"

10. **Ask for quality review**
    - Ask user if results look correct
    - If not, discuss iteration strategy (new version or new classifier)

11. **Offer annotation tool**
    - Remind user they can use `annotation_tool.html` for systematic review

### Example Dialogue

```
User: I want to classify emails by sentiment

Claude: [silently checks .tables, .schema, ai_classification_runs]

Claude: I'll create sentiment classifier C6:
- Input: `1_discussion_messages.body`
- Output: `sentiment [C6]` column (added to same table)
- Model: gemini-3.0-flash

Running now...
[progress bar: 100%|████████| 500/500]

Done. Here are 5 random samples:
[shows results]

Total cost: $0.42 (500 calls × ~0.8k tokens avg)

Do these look correct? You can also review more samples in annotation_tool.html
```

### Key Principle

**Be autonomous, not interrogative.** Analyze the schema, make smart decisions, propose a concrete plan. Only ask when there's genuine ambiguity that could lead to wasted work.

---

## Quick Reference: New Classifier Checklist

- [ ] Choose classifier ID (`CX`)
- [ ] Define input columns
- [ ] Define output columns with `[CX]` suffix
- [ ] Create SAMPLE table if testing (e.g., 500 rows)
- [ ] Create/modify table with `CLASSIFIER` prefix
- [ ] Write and test prompt
- [ ] Register in `ai_classification_runs`
- [ ] Run classifier with progress bar (tqdm)
- [ ] Export results to `data/classification/{run_id}_{run_name}.json`
- [ ] Track and report total API cost
- [ ] Update `total_cost` in registry
- [ ] Review sample of results
- [ ] Iterate if needed (version up)
- [ ] Run on full dataset once satisfied
